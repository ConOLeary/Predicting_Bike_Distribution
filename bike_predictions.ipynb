{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS & DEFINITIONS\n",
    "\n",
    "import csv, sys\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np; np.set_printoptions(threshold=sys.maxsize)\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "import math\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "DUD_VALUE= 0 # change from 0 to something like 123 for debugging\n",
    "EMPTY_DATA_DAY_VAL= 123456789\n",
    "TOTAL_ROWS= 9999999999\n",
    "INPUT_ROWS_LIMIT= TOTAL_ROWS # 500000\n",
    "FILENAME= 'dublinbikes_2020_Q1.csv'\n",
    "MAX_STATIONS= 118\n",
    "SECS_IN_5MIN= 300\n",
    "DATAPOINT_EVERYX_MIN= 5\n",
    "DATAPOINTS_PER_DAY= 288\n",
    "DAYS_OF_WEEK= ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'] # yes, I consider Monday to be the '0'/start of the week\n",
    "STARTING_DATE= 0 # aka Monday. Because the 27th of Jan 2020 is a Monday\n",
    "MISSING_STATIONS= [117, 116, 70, 60, 46, 35, 20, 14, 1, 0]\n",
    "NUM_STATIONS= MAX_STATIONS - len(MISSING_STATIONS)\n",
    "SUBSTANDARD_DAYS= [] # [50, 49]\n",
    "TOTAL_DAYS= 66 # from 27 / 1 / 2020 to (and including) 1 / 4 / 2020\n",
    "HOURS= 24\n",
    "EPOCH= datetime.datetime(2020, 1, 27, 0, 0)\n",
    "TOTAL_TIME_DATAPOINTS= int((datetime.datetime(2020,4,2,0,0) - EPOCH).total_seconds() / SECS_IN_5MIN)\n",
    "K= 5\n",
    "STEP_SIZE= 0.02185 # just the magic number that leads to 288 values being generated\n",
    "R= 0.5\n",
    "MAX_HINDSIGHT= 60 # minutes\n",
    "DAYS_PER_WEEKDAY= 5\n",
    "HOMEMADE_REGULISER= 0.8\n",
    "\n",
    "class DataDay: # ideally this would be nested in the Station class\n",
    "    def __init__(self, index):\n",
    "        self.index= index\n",
    "        self.substandard_day= False\n",
    "        if index in SUBSTANDARD_DAYS:\n",
    "            self.substandard_day= True\n",
    "        self.times_populated= 0\n",
    "        self.day_of_week= ((STARTING_DATE + index) % len(DAYS_OF_WEEK))\n",
    "        \n",
    "        self.daily_epoch_time= np.full(DATAPOINTS_PER_DAY, EMPTY_DATA_DAY_VAL, dtype=np.int)\n",
    "        self.epoch_time= np.full(DATAPOINTS_PER_DAY, EMPTY_DATA_DAY_VAL, dtype=np.int)\n",
    "        self.bikes= np.full(DATAPOINTS_PER_DAY, EMPTY_DATA_DAY_VAL, dtype=np.int)\n",
    "        self.percent_bikes= np.full(DATAPOINTS_PER_DAY, float(EMPTY_DATA_DAY_VAL), dtype=np.float)\n",
    "\n",
    "    def populate(self, daily_epoch_time, epoch_time, bikes, percent_bikes):\n",
    "        if self.substandard_day == False:\n",
    "            self.daily_epoch_time[daily_epoch_time]= daily_epoch_time\n",
    "            self.epoch_time[daily_epoch_time]= epoch_time\n",
    "            self.bikes[daily_epoch_time]= bikes\n",
    "            self.percent_bikes[daily_epoch_time]= percent_bikes\n",
    "            self.times_populated+= 1\n",
    "\n",
    "class Station:\n",
    "    def __init__(self, index):\n",
    "        self.index= index\n",
    "        self.name= DUD_VALUE\n",
    "        self.bike_capacity= DUD_VALUE\n",
    "        self.address= DUD_VALUE\n",
    "        self.latitude= DUD_VALUE\n",
    "        self.longitude= DUD_VALUE\n",
    "        self.data_days= [DataDay(i) for i in range(0, TOTAL_DAYS)]\n",
    "    \n",
    "    def populate_consts(self, name, bike_capacity, address, latitude, longitude):\n",
    "        self.name= name\n",
    "        self.bike_capacity= bike_capacity\n",
    "        self.address= address\n",
    "        self.latitude= latitude\n",
    "        self.longitude= longitude\n",
    "\n",
    "def get_station_id(name):\n",
    "    try:\n",
    "        index= [x.name for x in stations].index(name)\n",
    "    except ValueError:\n",
    "        index= -1\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# DATA STRUCTURING\n",
    "\n",
    "total_capacity= 0 # not in use currently\n",
    "index= []; daily_epoch_time= []; epoch_time= []; percent_bikes= [];\n",
    "stations= [Station(i) for i in range(0, MAX_STATIONS)]\n",
    "indices_to_populate= list(range(0, MAX_STATIONS))\n",
    "for index in MISSING_STATIONS:\n",
    "    indices_to_populate.remove(index)\n",
    "\n",
    "with open(FILENAME, newline='') as f:\n",
    "    reader = csv.reader(f); next(reader) # skip data header\n",
    "    current_index= 0\n",
    "    try:\n",
    "        while len(indices_to_populate) != 0:\n",
    "            row= next(reader)\n",
    "            if int(row[0]) == current_index: # this clause is just for performance\n",
    "                continue\n",
    "            current_index= int(row[0])\n",
    "            if current_index in indices_to_populate:\n",
    "                stations[current_index].populate_consts(row[3], row[4], row[8], row[9], row[10])\n",
    "                indices_to_populate.remove(current_index)\n",
    "                total_capacity+= int(row[4])\n",
    "        \n",
    "        f.seek(0)\n",
    "        reader= csv.reader(f); row= next(reader) # skip data header\n",
    "        for row_i, row in enumerate(reader):\n",
    "            if row_i >= INPUT_ROWS_LIMIT:\n",
    "                break\n",
    "            if int((datetime.datetime(int(row[1][0:4]), int(row[1][5:7]), int(row[1][8:10]), int(row[1][11: 13]), int(row[1][14: 16])) - EPOCH).total_seconds()) < 0:\n",
    "                continue\n",
    "            try:\n",
    "                epoch_time= int((datetime.datetime(int(row[1][0:4]), int(row[1][5:7]), int(row[1][8:10]), int(row[1][11: 13]), int(row[1][14: 16])) - EPOCH).total_seconds() / SECS_IN_5MIN)\n",
    "                stations[int(row[0])].data_days[int(epoch_time / DATAPOINTS_PER_DAY)].populate( \\\n",
    "                    int((datetime.datetime(int(row[1][0:4]), int(row[1][5:7]), int(row[1][8:10]), int(row[1][11: 13]), int(row[1][14: 16])) - datetime.datetime(int(row[1][0:4]), int(row[1][5:7]), int(row[1][8:10]), 0, 0)).total_seconds() / (SECS_IN_5MIN)), \\\n",
    "                    epoch_time, \\\n",
    "                    int(row[6]), \\\n",
    "                    float(\"{:.3f}\".format(float(row[6]) / float(row[4]))))\n",
    "            except IndexError as e:\n",
    "                print(\"Error:\", e, int(row[0]))\n",
    "                #print(\"\\nTRIED: \", epoch_time, ' / ', DATAPOINTS_PER_DAY, ' = ', int(epoch_time / DATAPOINTS_PER_DAY))\n",
    "                #print(row[1])\n",
    "    except csv.Error as e:\n",
    "        sys.exit('file {}, line {}: {}'.format(filename, reader.line_num, e))\n",
    "            \n",
    "for station_i, station in enumerate(stations):\n",
    "    last_bikes= 0\n",
    "    last_percent_bikes= 0\n",
    "    for day_i, data_day in enumerate(station.data_days):\n",
    "        for val_i, val in enumerate(data_day.bikes):\n",
    "            if val == EMPTY_DATA_DAY_VAL:\n",
    "                stations[station_i].data_days[day_i].populate(val_i, day_i * DATAPOINTS_PER_DAY + val_i, last_bikes, last_percent_bikes)\n",
    "            else:\n",
    "                last_bikes= data_day.bikes[val_i]\n",
    "                last_percent_bikes= data_day.percent_bikes[val_i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# FEATURE DATA PREPERATION\n",
    "\n",
    "fullness= np.full((TOTAL_TIME_DATAPOINTS, NUM_STATIONS), DUD_VALUE, dtype=np.int)\n",
    "fullness_in10= np.full((TOTAL_TIME_DATAPOINTS, NUM_STATIONS), DUD_VALUE, dtype=np.int)\n",
    "fullness_in30= np.full((TOTAL_TIME_DATAPOINTS, NUM_STATIONS), DUD_VALUE, dtype=np.int)\n",
    "fullness_in60= np.full((TOTAL_TIME_DATAPOINTS, NUM_STATIONS), DUD_VALUE, dtype=np.int)\n",
    "fullness_percent= np.full((TOTAL_TIME_DATAPOINTS, NUM_STATIONS), DUD_VALUE, dtype=np.float)\n",
    "bikes_changes_pastx= np.full((TOTAL_TIME_DATAPOINTS, NUM_STATIONS, int(MAX_HINDSIGHT / DATAPOINT_EVERYX_MIN)), DUD_VALUE, dtype=np.int)\n",
    "days_of_week= np.full((TOTAL_TIME_DATAPOINTS, len(DAYS_OF_WEEK)), DUD_VALUE, dtype=np.int)\n",
    "hour_of_day= np.full((TOTAL_TIME_DATAPOINTS, HOURS), DUD_VALUE, dtype=np.float)\n",
    "average_weekday_fullness= np.full((DATAPOINTS_PER_DAY, NUM_STATIONS, len(DAYS_OF_WEEK)), DUD_VALUE, dtype=np.float)\n",
    "weekdays_vol= np.full((NUM_STATIONS, len(DAYS_OF_WEEK)), 0, dtype=np.float)\n",
    "avrg_weekday_full= np.full((NUM_STATIONS, len(DAYS_OF_WEEK)), 0, dtype=np.float)\n",
    "meanmean= np.full(NUM_STATIONS, 0, dtype=np.float)\n",
    "\n",
    "station_index_decrement= 0 # this is a varying offset for the indexing of stations that accounts for missing stations that are being ignored\n",
    "for epoch_day_i in range(TOTAL_DAYS):\n",
    "    #print(\"########### epoch_day_i: \", epoch_day_i)\n",
    "    x_offset= epoch_day_i * DATAPOINTS_PER_DAY\n",
    "    y_offset= 0\n",
    "    \n",
    "    block= np.zeros((DATAPOINTS_PER_DAY, HOURS), dtype=np.float)\n",
    "    daily_epoch_time= list(range(DATAPOINTS_PER_DAY))\n",
    "    for time_i in daily_epoch_time:\n",
    "        hour= float(\"{:.3f}\".format(time_i / 12)) # divide by 12 because there are 12 datapoints in an hour\n",
    "        block[time_i][(int(hour) + 1) % HOURS]= hour % 1\n",
    "        block[time_i][int(hour)]= 1 - (hour % 1)\n",
    "    hour_of_day[x_offset:x_offset + block.shape[0], y_offset:y_offset + block.shape[1]]= block\n",
    "    \n",
    "    day_of_week= stations[2].data_days[epoch_day_i].day_of_week\n",
    "    block= np.zeros((DATAPOINTS_PER_DAY, len(DAYS_OF_WEEK)), dtype=np.int)\n",
    "    for block_i, sub_arr in enumerate(block):\n",
    "        block[block_i][day_of_week]= 1\n",
    "    days_of_week[x_offset:x_offset + block.shape[0], y_offset:y_offset + block.shape[1]]= block\n",
    "    \n",
    "    for station in stations:\n",
    "        #print(\"###### station.index: \", station.index)\n",
    "        if station.index == 0:\n",
    "            station_index_decrement= 0\n",
    "        if station.index in MISSING_STATIONS:\n",
    "            station_index_decrement+= 1\n",
    "            continue\n",
    "        y_offset= station.index - station_index_decrement\n",
    "        \n",
    "        block= station.data_days[epoch_day_i].percent_bikes\n",
    "        block= np.reshape(block, (DATAPOINTS_PER_DAY, 1))\n",
    "        fullness_percent[x_offset:x_offset + block.shape[0], y_offset:y_offset + block.shape[1]]= block\n",
    "        \n",
    "        block= station.data_days[epoch_day_i].bikes\n",
    "        block= np.reshape(block, (DATAPOINTS_PER_DAY, 1))\n",
    "        fullness[x_offset:x_offset + block.shape[0], y_offset:y_offset + block.shape[1]]= block\n",
    "        block= np.reshape(block, (DATAPOINTS_PER_DAY, 1, 1))\n",
    "        if weekdays_vol[y_offset, day_of_week] < DAYS_PER_WEEKDAY:\n",
    "            average_weekday_fullness[0:DATAPOINTS_PER_DAY, y_offset:y_offset + block.shape[1], day_of_week:day_of_week+1]+= block\n",
    "            weekdays_vol[y_offset:y_offset+1, day_of_week:day_of_week+1]+= 1\n",
    "        \n",
    "        bikes= station.data_days[epoch_day_i].bikes\n",
    "        block= np.reshape(bikes[2:], (bikes.shape[0] - 2, 1))\n",
    "        fullness_in10[x_offset:x_offset + block.shape[0], y_offset:y_offset + block.shape[1]]= block\n",
    "        block= np.reshape(bikes[6:], (bikes.shape[0] - 6, 1))\n",
    "        fullness_in30[x_offset:x_offset + block.shape[0], y_offset:y_offset + block.shape[1]]= block\n",
    "        block= np.reshape(bikes[12:], (bikes.shape[0] - 12, 1))\n",
    "        fullness_in60[x_offset:x_offset + block.shape[0], y_offset:y_offset + block.shape[1]]= block\n",
    "        \n",
    "        block= np.reshape(station.data_days[epoch_day_i].bikes, (DATAPOINTS_PER_DAY, 1))\n",
    "        if epoch_day_i - 1 == -1:\n",
    "            prev_block= np.zeros((DATAPOINTS_PER_DAY, 1), dtype=np.int)\n",
    "        else:\n",
    "            prev_block= np.reshape(station.data_days[epoch_day_i - 1].bikes, (DATAPOINTS_PER_DAY, 1))\n",
    "        block_xminchange= np.zeros((DATAPOINTS_PER_DAY, int(MAX_HINDSIGHT / DATAPOINT_EVERYX_MIN)), dtype=np.int)\n",
    "        fullness_xago= np.zeros((DATAPOINTS_PER_DAY, int(MAX_HINDSIGHT / DATAPOINT_EVERYX_MIN)), dtype=np.int)\n",
    "        for col_i in range(fullness_xago.shape[1]):\n",
    "            i= col_i + 1\n",
    "            fullness_xago[i:DATAPOINTS_PER_DAY, col_i:col_i + 1]= block[0:DATAPOINTS_PER_DAY - i, 0:1]\n",
    "            fullness_xago[0:i, col_i:col_i + 1]= prev_block[DATAPOINTS_PER_DAY - i:DATAPOINTS_PER_DAY, 0:1]\n",
    "        for col_i in range(fullness_xago.shape[1]):\n",
    "            block_xminchange[0:DATAPOINTS_PER_DAY, col_i:col_i + 1]= np.subtract(block, fullness_xago[0:DATAPOINTS_PER_DAY, col_i:col_i + 1])\n",
    "        \n",
    "        bikes_changes_pastx[x_offset:x_offset + block_xminchange.shape[0], y_offset:y_offset + 1, 0:block_xminchange.shape[1]]= np.reshape(block_xminchange, (DATAPOINTS_PER_DAY, 1, block_xminchange.shape[1]))\n",
    "\n",
    "station_index_decrement= 0 # this is a varying offset for the indexing of stations that accounts for missing stations that are being ignored\n",
    "for station in stations:\n",
    "    if station.index == 0: # [117, 116, 70, 60, 46, 35, 20, 14, 1, 0]\n",
    "        station_index_decrement= 0\n",
    "    if station.index in MISSING_STATIONS:\n",
    "        station_index_decrement+= 1\n",
    "        continue\n",
    "    y_offset= station.index - station_index_decrement\n",
    "    for day_of_week_i in range(len(DAYS_OF_WEEK)):\n",
    "        average_weekday_fullness[0:DATAPOINTS_PER_DAY, y_offset:y_offset+1, day_of_week_i:day_of_week_i+1]/= weekdays_vol[y_offset:y_offset+1, day_of_week_i:day_of_week_i+1]\n",
    "        avrg_weekday_full[y_offset:y_offset+1, day_of_week_i:day_of_week_i+1]= np.mean(average_weekday_fullness[0:DATAPOINTS_PER_DAY, y_offset:y_offset+1, day_of_week_i:day_of_week_i+1])\n",
    "    meanmean[y_offset:y_offset+1]= np.mean(avrg_weekday_full[y_offset:y_offset+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "# APPROACH DEFINITIONS\n",
    "    \n",
    "def run_approach1(station_name):\n",
    "    index= get_station_id(station_name)\n",
    "    \n",
    "    y= np.full((TOTAL_TIME_DATAPOINTS, 3), 0, dtype=np.int) # change the 3 to a 6 to do both stations at once on the generalised-training form of an approach\n",
    "    y[0:TOTAL_TIME_DATAPOINTS, 0:1]= np.reshape(fullness_in10[:,index], (TOTAL_TIME_DATAPOINTS, 1))\n",
    "    y[0:TOTAL_TIME_DATAPOINTS, 1:2]= np.reshape(fullness_in30[:,index], (TOTAL_TIME_DATAPOINTS, 1))\n",
    "    y[0:TOTAL_TIME_DATAPOINTS, 2:3]= np.reshape(fullness_in60[:,index], (TOTAL_TIME_DATAPOINTS, 1))\n",
    "    \n",
    "    X= np.full((TOTAL_TIME_DATAPOINTS, hour_of_day.shape[1] + days_of_week.shape[1] + 3 \\\n",
    "                + 0 * NUM_STATIONS \\\n",
    "               ), 0, dtype=np.float)\n",
    "    X[0:TOTAL_TIME_DATAPOINTS, 0:7]= day_of_week\n",
    "    X[0:TOTAL_TIME_DATAPOINTS, 7:31]= hour_of_day\n",
    "    X[0:TOTAL_TIME_DATAPOINTS, 31:32]= fullness_percent[0:TOTAL_TIME_DATAPOINTS, index:index + 1]\n",
    "    X[0:TOTAL_TIME_DATAPOINTS, 32:33]= np.reshape((bikes_changes_pastx[0:TOTAL_TIME_DATAPOINTS, index:index + 1, 0:1]), (TOTAL_TIME_DATAPOINTS, 1)) # past5\n",
    "    X[0:TOTAL_TIME_DATAPOINTS, 33:34]= np.reshape((bikes_changes_pastx[0:TOTAL_TIME_DATAPOINTS, index:index + 1, 1:2]), (TOTAL_TIME_DATAPOINTS, 1)) # past10\n",
    "    #X[0:TOTAL_TIME_DATAPOINTS, 34:35]= np.reshape((bikes_changes_pastx[0:TOTAL_TIME_DATAPOINTS, index:index+1, 2:3]), (TOTAL_TIME_DATAPOINTS, 1)) # past15\n",
    "    #X[0:TOTAL_TIME_DATAPOINTS, 35:36]= np.reshape((bikes_changes_pastx[0:TOTAL_TIME_DATAPOINTS, index:index+1, 3:4]), (TOTAL_TIME_DATAPOINTS, 1)) # past20\n",
    "    #X[0:TOTAL_TIME_DATAPOINTS, 36:37]= np.reshape((bikes_changes_pastx[0:TOTAL_TIME_DATAPOINTS, index:index+1, 4:5]), (TOTAL_TIME_DATAPOINTS, 1)) # past25\n",
    "    # X[0:TOTAL_TIME_DATAPOINTS, 139:247]= np.reshape((bikes_changes_pastx[0:TOTAL_TIME_DATAPOINTS, 0:NUM_STATIONS, 0:1]), (TOTAL_TIME_DATAPOINTS, 1)) # past5\n",
    "    # X[0:TOTAL_TIME_DATAPOINTS, 247:355]= np.reshape((bikes_changes_pastx[0:TOTAL_TIME_DATAPOINTS, 0:NUM_STATIONS, 2:3]), (TOTAL_TIME_DATAPOINTS, 1)) # past15\n",
    "    # X[0:TOTAL_TIME_DATAPOINTS, 355:463]= np.reshape((bikes_changes_pastx[0:TOTAL_TIME_DATAPOINTS, 0:NUM_STATIONS, 8:9]), (TOTAL_TIME_DATAPOINTS, 1)) # past45\n",
    "\n",
    "    kf= KFold(n_splits= K)\n",
    "    kf.get_n_splits(X)\n",
    "    score_sum= 0.0\n",
    "    i= 1\n",
    "    returns= []\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test= X[train_index], X[test_index]\n",
    "        y_train, y_test= y[train_index], y[test_index]\n",
    "        regr= MLPRegressor(random_state= 1, max_iter= 1000, alpha=0.001).fit(X_train, y_train)\n",
    "        y_pred= regr.predict(X_test)\n",
    "        score_sum+= regr.score(X_test, y_test)\n",
    "        returns.append(regr.score(X_test, y_test))\n",
    "        print(\"R**2 score of data split\", i, \": \", regr.score(X_test, y_test))\n",
    "        i+= 1\n",
    "    print(\"\\nAVERAGE R**2 score: \", score_sum / K)\n",
    "    return returns\n",
    "    \n",
    "def run_approach2(station_name):\n",
    "    index= get_station_id(station_name)\n",
    "    \n",
    "    y= np.full((TOTAL_TIME_DATAPOINTS, 3), 0, dtype=np.int) # change the 3 to a 6 to do both stations at once on the generalised-training form of an approach\n",
    "    y[0:TOTAL_TIME_DATAPOINTS, 0:1]= np.reshape(fullness_in10[:,index], (TOTAL_TIME_DATAPOINTS, 1))\n",
    "    y[0:TOTAL_TIME_DATAPOINTS, 1:2]= np.reshape(fullness_in30[:,index], (TOTAL_TIME_DATAPOINTS, 1))\n",
    "    y[0:TOTAL_TIME_DATAPOINTS, 2:3]= np.reshape(fullness_in60[:,index], (TOTAL_TIME_DATAPOINTS, 1))\n",
    "    \n",
    "    X= np.full((TOTAL_TIME_DATAPOINTS, 2 + 2 \\\n",
    "            #* bikes_changes_pastx.shape[1] \\ # This line is uncommented when training on all stations\n",
    "           ), -1, dtype=np.float)\n",
    "    \n",
    "    positions= []; t= 0\n",
    "    while t < 2 * math.pi:\n",
    "        positions.append((1 - (R * math.cos(t) + R), R * math.sin(t) + R))\n",
    "        t+= STEP_SIZE\n",
    "    pos_i= 0\n",
    "    for time_i in range(TOTAL_TIME_DATAPOINTS):\n",
    "        X[time_i, 0]= positions[pos_i][0]\n",
    "        X[time_i, 1]= positions[pos_i][1]\n",
    "        pos_i= (pos_i + 1) % len(positions)\n",
    "    \n",
    "    X[0:TOTAL_TIME_DATAPOINTS, 2:3]= fullness_percent[0:TOTAL_TIME_DATAPOINTS, index:index+1]\n",
    "    X[0:TOTAL_TIME_DATAPOINTS, 3:4]= np.reshape((bikes_changes_pastx[0:TOTAL_TIME_DATAPOINTS, index:index+1, 0:1]), (TOTAL_TIME_DATAPOINTS, 1)) # past5\n",
    "    # X[0:TOTAL_TIME_DATAPOINTS, 2:110]= bikes_changes_past5\n",
    "    # X[0:TOTAL_TIME_DATAPOINTS, 110:218]= bikes_changes_past15\n",
    "    \n",
    "    neigh= KNeighborsRegressor(n_neighbors= 30, weights='distance')\n",
    "    cv_scores= cross_val_score(neigh, X, y, cv=5)\n",
    "    print(cv_scores) # print each cv score (accuracy) and average them\n",
    "    print('cv_scores mean:{}'.format(np.mean(cv_scores)))\n",
    "    return np.mean(cv_scores)\n",
    "\n",
    "def run_oldbaseline(station_name, regulariser_coef):\n",
    "    index= get_station_id(station_name)\n",
    "    max_train_time= DATAPOINTS_PER_DAY * DAYS_PER_WEEKDAY * len(DAYS_OF_WEEK)\n",
    "    y_test= np.reshape(fullness[max_train_time:TOTAL_TIME_DATAPOINTS, index:index+1], TOTAL_TIME_DATAPOINTS - max_train_time)\n",
    "    y_pred= np.zeros(TOTAL_TIME_DATAPOINTS - max_train_time)\n",
    "    for i in range(int((TOTAL_TIME_DATAPOINTS - max_train_time) / DATAPOINTS_PER_DAY)):\n",
    "        datapoint_i= i * DATAPOINTS_PER_DAY\n",
    "        day_of_week_i= int((max_train_time + datapoint_i) / DATAPOINTS_PER_DAY) % len(DAYS_OF_WEEK)\n",
    "        y_pred[datapoint_i:datapoint_i + DATAPOINTS_PER_DAY]= (np.reshape(average_weekday_fullness[0:DATAPOINTS_PER_DAY, index:index+1, day_of_week_i:day_of_week_i+1], DATAPOINTS_PER_DAY) * (1 - regulariser_coef) + np.full(DATAPOINTS_PER_DAY, avrg_weekday_full[index:index+1, day_of_week_i:day_of_week_i+1]) * regulariser_coef)\n",
    "#     for val_i in range(y_pred.shape[0]):\n",
    "#         print(\"y_test:\",y_test[val_i],\" y_pred:\",y_pred[val_i])\n",
    "    print(\"R**2 score: \", r2_score(y_test, y_pred))\n",
    "    return r2_score(y_test, y_pred)\n",
    "\n",
    "def run_meanline(station_name):\n",
    "    index= get_station_id(station_name)\n",
    "    max_train_time= DATAPOINTS_PER_DAY * DAYS_PER_WEEKDAY * len(DAYS_OF_WEEK)\n",
    "    y_test= np.reshape(fullness[max_train_time:TOTAL_TIME_DATAPOINTS, index:index+1], TOTAL_TIME_DATAPOINTS - max_train_time)\n",
    "    y_pred= np.zeros(TOTAL_TIME_DATAPOINTS - max_train_time)\n",
    "    for i in range(int((TOTAL_TIME_DATAPOINTS - max_train_time) / DATAPOINTS_PER_DAY)):\n",
    "        datapoint_i= i * DATAPOINTS_PER_DAY\n",
    "        day_of_week_i= int((max_train_time + datapoint_i) / DATAPOINTS_PER_DAY) % len(DAYS_OF_WEEK)\n",
    "        y_pred[datapoint_i:datapoint_i + DATAPOINTS_PER_DAY]= np.full(DATAPOINTS_PER_DAY, meanmean[index:index+1], dtype=np.float64)\n",
    "#     for val_i in range(y_pred.shape[0]):\n",
    "#         print(\"y_test:\",y_test[val_i],\" y_pred:\",y_pred[val_i])\n",
    "    print(\"R**2 score: \", r2_score(y_test, y_pred))\n",
    "    return r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R**2 score of data split 1 :  0.9194040596091971\n",
      "R**2 score of data split 2 :  0.9417536913111775\n",
      "R**2 score of data split 3 :  0.9195457533923225\n",
      "R**2 score of data split 4 :  0.9198762534756\n",
      "R**2 score of data split 5 :  0.8838514091695006\n",
      "\n",
      "AVERAGE R**2 score:  0.9168862333915596\n",
      "--------------------\n",
      "R**2 score of data split 1 :  0.7946041453048031\n",
      "R**2 score of data split 2 :  0.8116244841170337\n",
      "R**2 score of data split 3 :  0.8236332524745844\n",
      "R**2 score of data split 4 :  0.8641826339236979\n",
      "R**2 score of data split 5 :  0.7858446675838877\n",
      "\n",
      "AVERAGE R**2 score:  0.8159778366808013\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8159778366808013"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DRIVER\n",
    "\n",
    "run_approach1(\"PORTOBELLO ROAD\") # HOMEMADE_REGULISER\n",
    "print(\"--------------------\")\n",
    "run_approach1(\"CUSTOM HOUSE QUAY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_approaches(station_name1, station_name2, approach1, approach2):\n",
    "    s1a1_r2s= []; s2a1_r2s= []; s1a2_r2s= []; s2a2_r2s= []\n",
    "    s1a1_r2s.append(approach1(station_name1)) # station 1, approach 1, r**2 score\n",
    "    s2a1_r2s.append(approach1(station_name2))\n",
    "    s1a2_r2s.append(approach2(station_name1))\n",
    "    s2a2_r2s.append(approach2(station_name2))\n",
    "    \n",
    "    \n",
    "    for i in range(len(s1a1_r2)):\n",
    "        s1_r2.append(run_oldbaseline(\"PORTOBELLO ROAD\", coef))\n",
    "        s2_r2.append(run_oldbaseline(\"CUSTOM HOUSE QUAY\", coef))\n",
    "        s1_r2meanmean.append(meanmean1)\n",
    "        s2_r2meanmean.append(meanmean2)\n",
    "\n",
    "    ax= plt.gca()\n",
    "\n",
    "    ax.plot(coefs, s1_r2, label=\"Portobello Road (baseline)\", color=\"#F28C28\")\n",
    "    ax.plot(coefs, s1_r2meanmean, label=\"Portobello Road (mean)\", color=\"#FAD5A5\")\n",
    "    ax.plot(coefs, s2_r2, label=\"Custom House Quay (baseline)\", color=\"#0047AB\")\n",
    "    ax.plot(coefs, s2_r2meanmean, label=\"Custom House Quay (mean)\", color=\"#A7C7E7\")\n",
    "\n",
    "    # Place a legend to the right of this smaller subplot.\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
    "\n",
    "    plt.xlabel('Baseline\\'s coefficent for homemade regulariser')\n",
    "    plt.ylabel('R**2 score')\n",
    "    plt.title('Baseline model')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
